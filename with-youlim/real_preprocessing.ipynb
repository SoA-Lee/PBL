{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "996beeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import json\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dacd62d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 크기 : \n",
      "real_refined_data.csv         1.12MB\n",
      "real_test.csv                 0.75MB\n",
      "real_test_data.csv            0.56MB\n",
      "real_test_label.csv           0.02MB\n",
      "real_train.csv                0.76MB\n",
      "real_train_data.csv           0.56MB\n"
     ]
    }
   ],
   "source": [
    "DATA_IN_PATH = './data_in/'\n",
    "print(\"파일 크기 : \")\n",
    "for file in os.listdir(DATA_IN_PATH):\n",
    "    if 'csv' in file :\n",
    "        print(file.ljust(30) + str(round(os.path.getsize(DATA_IN_PATH + file) / 1000000, 2)) + 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d95fb430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>그럼 회의를 열어서 신입을 모집할지 경력직을 모집할지 정합시다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>상황 해결을 위해 이번 분기에 경력직을 모집하는 건 어떨까요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>다양한 경험을 가진 분이 온다면 상황을 해결하는 데 큰 도움을 줄 것 같습니다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>그럼 다음 달에 홈페이지와 신문에 모집 공고를 띄울까요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>최대한 빨리 공고를 띄워서 이번 달 안에 선발 부탁드립니다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      sentence  label\n",
       "0           그럼 회의를 열어서 신입을 모집할지 경력직을 모집할지 정합시다      0\n",
       "1            상황 해결을 위해 이번 분기에 경력직을 모집하는 건 어떨까요      0\n",
       "2  다양한 경험을 가진 분이 온다면 상황을 해결하는 데 큰 도움을 줄 것 같습니다      0\n",
       "3               그럼 다음 달에 홈페이지와 신문에 모집 공고를 띄울까요      0\n",
       "4             최대한 빨리 공고를 띄워서 이번 달 안에 선발 부탁드립니다      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(DATA_IN_PATH+'real_train_data.csv', header = 0)\n",
    "test_data = pd.read_csv(DATA_IN_PATH+'real_test_data.csv', header = 0)\n",
    "train_data.head()\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a80346e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 긍정 리뷰 개수: 3252\n",
      "train 부정 리뷰 개수: 3252\n",
      "test 긍정 리뷰 개수: 3253\n",
      "test 부정 리뷰 개수: 3253\n"
     ]
    }
   ],
   "source": [
    "print(\"train 긍정 리뷰 개수: {}\".format(train_data['label'].value_counts()[0]))\n",
    "print(\"train 부정 리뷰 개수: {}\".format(train_data['label'].value_counts()[1]))\n",
    "print(\"test 긍정 리뷰 개수: {}\".format(test_data['label'].value_counts()[0]))\n",
    "print(\"test 부정 리뷰 개수: {}\".format(test_data['label'].value_counts()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "117ac5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentence, okt, remove_stopwords = False, stop_words = []):\n",
    "    # 함수의 인자는 다음과 같다.\n",
    "    # sentence : 전처리할 텍스트\n",
    "    # okt : okt 객체를 반복적으로 생성하지 않고 미리 생성후 인자로 받는다.\n",
    "    # remove_stopword : 불용어를 제거할지 선택 기본값은 False\n",
    "    # stop_word : 불용어 사전은 사용자가 직접 입력해야함 기본값은 비어있는 리스트\n",
    "    \n",
    "    # 1. 한글 및 공백을 제외한 문자 모두 제거.\n",
    "    sentence_text = re.sub(\"[ㄱ-ㅎㅏ-ㅣ\\\\s]+\", \"\", sentence) \n",
    "    \n",
    "    \n",
    "    # 2. okt 객체를 활용해서 형태소 단위로 나눈다.\n",
    "    wd_sentence = okt.morphs(sentence_text, stem=True)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        \n",
    "        # 불용어 제거(선택적)\n",
    "        wd_sentence = [token for token in wd_sentence if not token in stop_words]\n",
    "        \n",
    "   \n",
    "    return wd_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c43fa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [ '은', '는', '이', '가', '하', '아', '것', '들','의', '있', '되', '수', '보', '주', '등', '한']\n",
    "okt = Okt()\n",
    "clean_train_sentence = []\n",
    "\n",
    "for review in train_data['sentence']:\n",
    "    # 비어있는 데이터에서 멈추지 않도록 string인 경우만 진행\n",
    "    if type(review) == str:\n",
    "        clean_train_sentence.append(preprocessing(review, okt, remove_stopwords = True, stop_words=stop_words))\n",
    "    else:\n",
    "        clean_train_sentence.append([])  #string이 아니면 비어있는 값 추가\n",
    "clean_train_sentence[:4]\n",
    "\n",
    "clean_train_df = pd.DataFrame({'sentence':clean_train_sentence,'label':train_data['label']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7555566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data = pd.read_csv(DATA_IN_PATH + 'testdata.csv', headear = 0)\n",
    "#test_data = pd.read_csv(DATA_IN_PATH + 'testdata.csv', names=['sentence'] )\n",
    "\n",
    "clean_test_sentence = []\n",
    "\n",
    "for review in test_data['sentence']:\n",
    "    # 비어있는 데이터에서 멈추지 않도록 string인 경우만 진행\n",
    "    if type(review) == str:\n",
    "        clean_test_sentence.append(preprocessing(review, okt, remove_stopwords = True, stop_words=stop_words))\n",
    "    else:\n",
    "        clean_test_sentence.append([])  #string이 아니면 비어있는 값 추가\n",
    "        \n",
    "clean_test_onlysentence_df = pd.DataFrame({'sentence':clean_test_sentence})\n",
    "clean_test_onlylabel_df = pd.DataFrame({'label':test_data['label']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c10df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_train_sentence) ###only train data만 단어사전에 \n",
    "train_sequences = tokenizer.texts_to_sequences(clean_train_sentence)\n",
    "test_sequences = tokenizer.texts_to_sequences(clean_test_sentence)\n",
    "\n",
    "word_vocab = tokenizer.word_index # 단어 사전 형태\n",
    "# word_vocab[\"<PAD>\"] = 0\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 30 #문장 최대 길이, 중간값 지정\n",
    "\n",
    "train_inputs = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post') #train data벡터화\n",
    "train_labels = np.array(train_data['label'])\n",
    "test_inputs = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post') #test data 벡터화\n",
    "test_labels = np.array(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eee6d6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './data_in/'\n",
    "from collections import OrderedDict\n",
    "\n",
    "TRAIN_INPUT_DATA = 'real_train_input.npy'\n",
    "TRAIN_LABEL_DATA = 'real_train_label.npy'\n",
    "TRAIN_CLEAN_DATA = 'real_train.csv'\n",
    "\n",
    "TEST_INPUT_DATA = 'real_test_input.npy'\n",
    "TEST_CLEAN_DATA = 'real_test.csv'\n",
    "TEST_CLEAN_LABEL = 'real_test_label.csv'\n",
    "TEST_LABEL_DATA = 'real_test_label.npy'\n",
    "\n",
    "DATA_CONFIGS = 'real_data_configs.json'\n",
    "\n",
    "# data_configs = {}\n",
    "data_configs = OrderedDict()\n",
    "\n",
    "data_configs['vocab'] = word_vocab\n",
    "#data_configs['vocab_size'] = len(word_vocab) # vocab size 추가\n",
    "data_configs['vocab_size'] = len(word_vocab)+1\n",
    "\n",
    "import os\n",
    "# 저장하는 디렉토리가 존재하지 않으면 생성\n",
    "if not os.path.exists(DATA_IN_PATH):\n",
    "    os.makedirs(DATA_IN_PATH)\n",
    "\n",
    "# 전처리 된 학습 데이터를 넘파이 형태로 저장\n",
    "np.save(open(DATA_IN_PATH + TRAIN_INPUT_DATA, 'wb'), train_inputs)\n",
    "np.save(open(DATA_IN_PATH + TRAIN_LABEL_DATA, 'wb'), train_labels)\n",
    "\n",
    "# 전처리 된 학습 데이터를 csv 형태로 저장\n",
    "clean_train_df.to_csv(DATA_IN_PATH +TRAIN_CLEAN_DATA, index = False)\n",
    "clean_test_onlysentence_df.to_csv(DATA_IN_PATH +TEST_CLEAN_DATA, index = False)\n",
    "clean_test_onlylabel_df.to_csv(DATA_IN_PATH + TEST_CLEAN_LABEL, index = False )\n",
    "\n",
    "# 전처리 된 테스트 데이터를 넘파이 형태로 저장\n",
    "np.save(open(DATA_IN_PATH + TEST_INPUT_DATA, 'wb'), test_inputs)\n",
    "np.save(open(DATA_IN_PATH + TEST_LABEL_DATA, 'wb'), test_labels)\n",
    "\n",
    "# 데이터 사전을 json 형태로 저장\n",
    "#json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, 'w',encoding='UTF-8'), ensure_ascii=False)\n",
    "\n",
    "#json 파일 생성\n",
    "with open(DATA_IN_PATH + DATA_CONFIGS, 'w', encoding='UTF-8') as f:\n",
    "    json_data = json.dumps(data_configs)\n",
    "    json_decode_data = json_data.encode('utf8').decode('unicode-escape')\n",
    "    f.write(json_decode_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
